[["index.html", "Untitled Chapter 1 Welcome", " Untitled Berrit Kievith 2023-04-25 Chapter 1 Welcome welcome to my bookdown website. This website contains my portfolio excercises and my cv. you can look throuh the pages with the search bar on the right bookdown::render_book() "],["ic50-c.elegans-experiment-workflow.html", "Chapter 2 IC50 C.elegans experiment workflow", " Chapter 2 IC50 C.elegans experiment workflow C.elegans plate experiment Data for this experiment was provided by J.Louter (INT/ILC). In the experiment adult C.elegans nematodes were incubated by different concentration of different chemicals. after the incubation the amount of offspring was counted. Ethanol was used as a positive control and S-medium as the negative control. For a full IC50 analysis the averages of the compounds should be merged. this can be plotted out in a line graph. then at the middle between the lowest and highest point on that spot on the X-axis is the IC50. after doing this for all the chemicals they can be compared. first we open and save the data file to our R console. library(readxl) library(tidyverse) onderzoek_data_portfolie_1 &lt;- read_xlsx(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) now that we imported the data we can check if the data types are rightfully interpreted. by using the head command we can see the first 10 rows and the column names and their assigned types. head(onderzoek_data_portfolie_1) ## # A tibble: 6 × 34 ## plateRow plateColumn vialNr dropCode expType expReplicate expName ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NA NA 1 a experiment 3 CE.LIQ.FLOW.062 ## 2 NA NA 1 b experiment 3 CE.LIQ.FLOW.062 ## 3 NA NA 1 c experiment 3 CE.LIQ.FLOW.062 ## 4 NA NA 1 d experiment 3 CE.LIQ.FLOW.062 ## 5 NA NA 1 e experiment 3 CE.LIQ.FLOW.062 ## 6 NA NA 2 a experiment 3 CE.LIQ.FLOW.062 ## # ℹ 27 more variables: expDate &lt;dttm&gt;, expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;, ## # expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;dbl&gt;, compCASRN &lt;chr&gt;, ## # compName &lt;chr&gt;, compConcentration &lt;chr&gt;, compUnit &lt;chr&gt;, ## # compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;, ## # elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, ## # bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, ## # bacterialVolUnit &lt;chr&gt;, incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, … in the data above we can see that Rawdata is dbl but this should be integer. compname is character but should be a factor and compconcentration is character but should be dbl before we can work with the data we are going to change these to the right data type. #install.packages(&quot;ggplot2&quot;) onderzoek_data_portfolie_1$compName &lt;- as.factor(onderzoek_data_portfolie_1$compName) onderzoek_data_portfolie_1$RawData &lt;- as.numeric(onderzoek_data_portfolie_1$RawData) onderzoek_data_portfolie_1$compConcentration &lt;- as.numeric(onderzoek_data_portfolie_1$compConcentration) head(onderzoek_data_portfolie_1) ## # A tibble: 6 × 34 ## plateRow plateColumn vialNr dropCode expType expReplicate expName ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NA NA 1 a experiment 3 CE.LIQ.FLOW.062 ## 2 NA NA 1 b experiment 3 CE.LIQ.FLOW.062 ## 3 NA NA 1 c experiment 3 CE.LIQ.FLOW.062 ## 4 NA NA 1 d experiment 3 CE.LIQ.FLOW.062 ## 5 NA NA 1 e experiment 3 CE.LIQ.FLOW.062 ## 6 NA NA 2 a experiment 3 CE.LIQ.FLOW.062 ## # ℹ 27 more variables: expDate &lt;dttm&gt;, expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;, ## # expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;dbl&gt;, compCASRN &lt;chr&gt;, ## # compName &lt;fct&gt;, compConcentration &lt;dbl&gt;, compUnit &lt;chr&gt;, ## # compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;, ## # elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, ## # bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, ## # bacterialVolUnit &lt;chr&gt;, incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, … By using the head command again we can see we have changed the data types. Next we are going to plot the data. we want the component concentration on the X-axis and the number of offspring on the Y-axis. for better viability we give the different components colours and the experimental conditions a different shape library(ggplot2) ggplot(onderzoek_data_portfolie_1, aes(x = compConcentration , y = RawData, colour = compName , shape = expType )) + geom_point() + labs(x = &quot;component concentration (nM)&quot; , y = &quot;number of offspring&quot; ) hmmm.. that does not tell us a whole lot about the data now does it. lets add a log10 transformation to the X-axis to even it out more. next we are going to add some jitter to points just to see the individual points better. library(ggplot2) ggplot(onderzoek_data_portfolie_1, aes(x = log10(compConcentration) , y = RawData, colour = compName , shape = expType )) + geom_jitter(width = 0.1) + labs(x = &quot;component concentration on log10 scale (nM)&quot; , y = &quot;number of offspring&quot; ) + xlim(-4.5 , 2) To really understand the effects of each component we are going to normalize the data. first we make the mean value of the negative control equal to one. then we will convert the other data to be a fraction of the negative control. we make a new vector with the new data and plot this like we did before. #view(onderzoek_data_portfolie_1) ngative_stuff &lt;- dplyr::filter(onderzoek_data_portfolie_1 ,expType == &quot;controlNegative&quot; ) mean(ngative_stuff$RawData) ## [1] 85.9 onderzoek_data_portfolie_1_newraw &lt;- onderzoek_data_portfolie_1 onderzoek_data_portfolie_1_newraw$RawData &lt;- onderzoek_data_portfolie_1_newraw$RawData/mean(ngative_stuff$RawData) ggplot(onderzoek_data_portfolie_1_newraw, aes(x = log10(compConcentration) , y = RawData, colour = compName , shape = expType )) + geom_jitter(width = 0.1) + labs(x = &quot;component concentration on log10 scale (nM)&quot; , y = &quot;number of offspring&quot; ) + xlim(-4.5 , 2) "],["map-tree.html", "Chapter 3 map tree", " Chapter 3 map tree fs::dir_tree(&quot;data/daur2_tidy_data&quot;) ## data/daur2_tidy_data ## ├── metagenomics_species_identification ## │ ├── metagenomics_excersises ## │ │ ├── analysis ## │ │ │ └── Metagenomics_exercise.Rmd ## │ │ └── data ## │ │ ├── bracken ## │ │ │ ├── mock1.bracken ## │ │ │ └── mock1_bracken_species.biom ## │ │ ├── fastqc ## │ │ │ ├── HU1_MOCK1_L001_R1_001_fastqc.html ## │ │ │ ├── HU1_MOCK1_L001_R1_001_fastqc.zip ## │ │ │ ├── HU1_MOCK1_L001_R2_001_fastqc.html ## │ │ │ ├── HU1_MOCK1_L001_R2_001_fastqc.zip ## │ │ │ └── README ## │ │ ├── fastQC_quality_images ## │ │ │ ├── quality_HU1_MOCK1_L001_R1_001.png ## │ │ │ ├── quality_HU1_MOCK1_L001_R2_001.png ## │ │ │ └── README ## │ │ └── kraken2 ## │ │ ├── mock1.report ## │ │ └── mock1_bracken_species.report ## │ ├── metagenomics_formatieve_opdracht ## │ │ ├── analysis ## │ │ │ └── formatieve_opdracht.Rmd ## │ │ └── data ## │ │ ├── fastQC_files ## │ │ │ ├── HU2_MOCK2_L001_R1_001_fastqc.html ## │ │ │ ├── HU2_MOCK2_L001_R1_001_fastqc.zip ## │ │ │ ├── HU2_MOCK2_L001_R2_001_fastqc.html ## │ │ │ ├── HU2_MOCK2_L001_R2_001_fastqc.zip ## │ │ │ └── README ## │ │ └── fastQC_quality_pictures ## │ │ ├── HU2_MOCK2_L001_R1_001.png ## │ │ ├── HU2_MOCK2_L001_R2_001.png ## │ │ └── README ## │ └── setup_metafile ## │ └── setup_meta_env.yml ## └── RNA_sequencing ## ├── airflow_practice_excercise ## │ ├── downloadfastq_command.sh ## │ ├── fastq_files ## │ ├── les2.Rproj ## │ ├── les2R.R ## │ ├── les3.Rproj ## │ ├── les3R.R ## │ └── les4R.R ## ├── iPSC_formatieve_opdracht ## │ ├── analyse ## │ │ └── iPSC_assesment_BerritKievith.Rmd ## │ ├── figures ## │ │ ├── airway_summary.PNG ## │ │ └── README ## │ ├── iPSC.Rproj ## │ └── suporting_infromation ## │ ├── airway_data.R ## │ ├── iPSC_notes.R ## │ └── README ## └── OneCut_eindopdracht ## ├── analysis ## │ └── eindopdracht_Onecut.Rmd ## ├── count_tabel ## ├── eindopdracht.Rproj ## └── fastQC_quality_images ## ├── quality_seq_SRR7866699_1.png ## ├── quality_SRR7866699_1.png ## ├── quality_SRR7866699_2.png ## ├── quality_SRR7866700_1.png ## ├── quality_SRR7866700_2.png ## ├── quality_SRR7866703_1.png ## ├── quality_SRR7866703_2.png ## ├── quality_SRR7866704_1.png ## ├── quality_SRR7866704_2.png ## └── README "],["the-importance-of-reproducible-data..html", "Chapter 4 The importance of reproducible data. 4.1 the second part of data", " Chapter 4 The importance of reproducible data. In this mark down i will show you the importance of clear and readable data. For this I have selected an open source research with experimental data. I have used this article: “Generalized additive models to analyze non-linear trends in biomedical longitudinal data using R: Beyond repeated measures ANOVA and Linear Mixed Models” https://doi.org/10.1101/2021.06.10.447970. his article explorers the uses of repeated measures analysis of variance (rm-ANOVA), linear mixed models (LMEMs) and Generalized additive models (GAMs). it shows that rm-ANOVA and LMEMs are used to look for linear trends is gathered data, But in most biomedical research there are no linear trends and the use of these Techniques can lead to biased conclusions. In contrast, GAMs do not assume linear trends in data and is there for more trust worthy the visualize this with simulated data about the oxygen saturation in tumors. I have ranked this article according to the following Transparency Criteria. Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. TRUE Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. TRUE (it is not a section but it is mentiond where to find the data.) Data Location Where the article’s data can be accessed, either raw or processed. https://github.com/aimundo/GAMs-biomedical-research a github repository. Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. FALSE: Author Review The professionalism of the contact information that the author has provided in the manuscript. TRUE Ariel I. Mundo1, Timothy J. Muldoon1* and John R. Tipton2 Corresponding author; email: tmuldoon@uark.edu Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. FALSE Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. TRUE Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. TRUE The article ranks good on most of the points the criteria, 4.1 the second part of data For the second part of looking at reproducing papers we are going to look at actual R code. the paper used is the same as in the previous page.first we take a look at the data https://doi.org/10.1101/2021.06.10.447970 The code used in the paper is spread over multiple R scripts this makes them easier to distinguish them from each other. The codes are not super complicated and where needed they give a comment about what the code is needed for. I would give this code a 4.4 its overall very good I would separate a few of the functions into separated R chunks but that is more of a personal preference Next we are going to run the code and try to recreate the following picture (FILL IN EXAMPLE PICTURE) Voorbeeld figuur remaking this was not super difficult in the main_manuscript file they gave all the packages needed to run the code as well as the seed they used. #the packages needed to run the code library(patchwork) library(tidyverse) library(mvnfast) ## ## Attaching package: &#39;mvnfast&#39; ## The following object is masked from &#39;package:lubridate&#39;: ## ## ms library(nlme) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse library(mgcv) ## This is mgcv 1.8-42. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. ## ## Attaching package: &#39;mgcv&#39; ## The following objects are masked from &#39;package:mvnfast&#39;: ## ## dmvn, rmvn library(gratia) library(scico) set.seed(2021) #set seed for reproducibility thm1&lt;-scale_fill_scico_d(palette=&quot;tokyo&quot;,begin=0.3, end=0.8, direction = -1, aesthetics = c(&quot;colour&quot;,&quot;fill&quot;)) ## This function plots the rm-ANOVA and LMEM for the data simulated in example.R plot_example &lt;- function(sim_dat, option=&quot;simple&quot;) { txt&lt;-20 #plot simulated data (scatterplot) p1 &lt;- sim_dat$dat %&gt;% ggplot(aes(x = time, y = y, group = treatment, color = treatment) ) + geom_point(show.legend=FALSE, alpha = 0.5) + labs(y=&#39;response&#39;)+ geom_line(aes(x = time, y = mu, color = treatment), show.legend=FALSE) + theme_classic() + theme(plot.title = element_text(size = txt, face = &quot;bold&quot;), text=element_text(size=txt))+ thm1 #plot the simulated data with trajectories per each subject p2 &lt;- sim_dat$dat %&gt;% ggplot(aes(x = time, y = y, group = subject, color = treatment) ) + geom_line(aes(size = &quot;Subjects&quot;), show.legend = FALSE) + # facet_wrap(~ treatment) + geom_line(aes(x = time, y = mu, color = treatment, size = &quot;Simulated Truth&quot;), lty = 1,show.legend = FALSE) + labs(y=&#39;response&#39;)+ scale_size_manual(name = &quot;Type&quot;, values=c(&quot;Subjects&quot; = 0.5, &quot;Simulated Truth&quot; = 3)) + theme_classic()+ theme(plot.title = element_text(size = txt, face = &quot;bold&quot;), text=element_text(size=txt))+ thm1 #plot the errors p3 &lt;- sim_dat$dat %&gt;% ggplot(aes(x = time, y = errors, group = subject, color = treatment)) + geom_line(show.legend=FALSE) + labs(y=&#39;errors&#39;)+ theme_classic()+ theme(plot.title = element_text(size = txt, face = &quot;bold&quot;), text=element_text(size=txt))+ thm1 #plot the model predictions for rm-ANOVA p4 &lt;- ggplot(sim_dat$dat, aes(x = time, y = y, color = treatment)) + geom_point(show.legend = FALSE, alpha=0.5)+ labs(y=&#39;response&#39;)+ geom_line(aes(y = predict(sim_dat$fit_anova), group = subject, size = &quot;Subjects&quot;),show.legend = FALSE) + geom_line(data = sim_dat$pred_dat, aes(y = predict(sim_dat$fit_anova, level = 0, newdata = sim_dat$pred_dat), size = &quot;Population&quot;), show.legend=FALSE) + guides(color = guide_legend(override.aes = list(size = 2)))+ scale_size_manual(name = &quot;Predictions&quot;, values=c(&quot;Subjects&quot; = 0.5, &quot;Population&quot; = 3)) + theme_classic() + theme(plot.title = element_text(size = txt, face = &quot;bold&quot;), text=element_text(size=txt))+ thm1 #plot the LMEM predictions p5 &lt;- ggplot(sim_dat$dat, aes(x = time, y = y, color = treatment)) + geom_point(alpha = 0.5)+ labs(y=&#39;response&#39;)+ geom_line(aes(y = predict(sim_dat$fit_lme), group = subject, size = &quot;Subjects&quot;)) + geom_line(data = sim_dat$pred_dat, aes(y = predict(sim_dat$fit_lme, level = 0, newdata = sim_dat$pred_dat), size = &quot;Population&quot;)) + guides(color = guide_legend(override.aes = list(size = 2)))+ scale_size_manual(name = &quot;Predictions&quot;, values=c(&quot;Subjects&quot; = 0.5, &quot;Population&quot; = 3)) + theme_classic() + theme(plot.title = element_text(size = txt, face = &quot;bold&quot;), text=element_text(size=txt))+ thm1 if(option==&#39;simple&#39;){ return((p1+p4+p5)+plot_layout(nrow=1)+plot_annotation(tag_levels = &#39;A&#39;)) } else { return((p1+p3+p2+p4+p5)+plot_layout(nrow=1)+plot_annotation(tag_levels = &#39;A&#39;)) } } ## Example with linear response. This function generates either linear or quadratic mean #responses with correlated or uncorrelated errors and fits a linear model to the data. example &lt;- function(n_time = 6, #number of time points fun_type = &quot;linear&quot;, #type of response error_type = &quot;correlated&quot;) { if (!(fun_type %in% c(&quot;linear&quot;, &quot;quadratic&quot;))) stop(&#39;fun_type must be either &quot;linear&quot;, or &quot;quadratic&quot;&#39;) if (!(error_type %in% c(&quot;correlated&quot;, &quot;independent&quot;))) stop(&#39;fun_type must be either &quot;correlated&quot;, or &quot;independent&quot;&#39;) x &lt;- seq(1,6, length.out = n_time) #Create mean response matrix: linear or quadratic mu &lt;- matrix(0, length(x), 2) # linear response if (fun_type == &quot;linear&quot;) { mu[, 1] &lt;- - (0.25*x)+2 mu[, 2] &lt;- 0.25*x+2 } else { # quadratic response (non-linear) mu[, 1] &lt;- -(0.25 * x^2) +1.5*x-1.25 mu[, 2] &lt;- (0.25 * x^2) -1.5*x+1.25 } #create an array where individual observations per each time point for each group are to be stored. Currently using 10 observations per timepoint y &lt;- array(0, dim = c(length(x), 2, 10)) #Create array to store the &quot;errors&quot; for each group at each timepoint. The &quot;errors&quot; are the #between-group variability in the response. errors &lt;- array(0, dim = c(length(x), 2, 10)) #create an array where 10 observations per each time point for each group are to be stored #The following cycles create independent or correlated responses. To each value of mu (mean response per group) a randomly generated error (correlated or uncorrelated) is added and thus the individual response is created. if (error_type == &quot;independent&quot;) { ## independent errors for (i in 1:2) { for (j in 1:10) { errors[, i, j] &lt;- rnorm(6, 0, 0.25) y[, i, j] &lt;- mu[, i] + errors[, i, j] } } } else { for (i in 1:2) { # number of treatments for (j in 1:10) { # number of subjects # compound symmetry errors: variance covariance matrix errors[, i, j] &lt;- rmvn(1, rep(0, length(x)), 0.1 * diag(6) + 0.25 * matrix(1, 6, 6)) y[, i, j] &lt;- mu[, i] + errors[, i, j] } } } ## subject random effects ## visualizing the difference between independent errors and compound symmetry ## why do we need to account for this -- overly confident inference #labelling y and errors dimnames(y) &lt;- list(time = x, treatment = 1:2, subject = 1:10) dimnames(errors) &lt;- list(time = x, treatment = 1:2, subject = 1:10) #labeling the mean response dimnames(mu) &lt;- list(time = x, treatment = 1:2) #convert y, mu and errors to dataframes with time, treatment and subject columns dat &lt;- as.data.frame.table(y, responseName = &quot;y&quot;) dat_errors &lt;- as.data.frame.table(errors, responseName = &quot;errors&quot;) dat_mu &lt;- as.data.frame.table(mu, responseName = &quot;mu&quot;) #join the dataframes to show mean response and errors per subject dat &lt;- left_join(dat, dat_errors, by = c(&quot;time&quot;, &quot;treatment&quot;, &quot;subject&quot;)) dat &lt;- left_join(dat, dat_mu, by = c(&quot;time&quot;, &quot;treatment&quot;)) #add time dat$time &lt;- as.numeric(as.character(dat$time)) #label subjects per group dat &lt;- dat %&gt;% mutate(subject = factor(paste(subject, treatment, sep = &quot;-&quot;))) ## repeated measures ANOVA in R #time and treatment interaction model fit_anova &lt;- lm(y ~ time + treatment + time * treatment, data = dat) #LMEM with compound symmetry fit_lme &lt;- lme(y ~ treatment + time + treatment:time, data = dat, random = ~ 1 | subject, correlation = corCompSymm(form = ~ 1 | subject) ) #create a prediction frame where the model can be used for plotting purposes pred_dat &lt;- expand.grid( treatment = factor(1:2), time = unique(dat$time) ) #add model predictions to the dataframe that has the simulated data dat$pred_anova &lt;- predict(fit_anova) dat$pred_lmem &lt;- predict(fit_lme) #return everything in a list return(list( dat = dat, pred_dat = pred_dat, fit_lme = fit_lme, fit_anova=fit_anova )) } Figure 4.1: Simulated responses from two groups with correlated errors using a LMEM and a rm-ANOVA model. Top row: linear response, bottom row: quadratic response. A: Simulated linear data with known mean response (thick lines) and individual responses (points) showing the dispersion of the data. D: Simulated quadratic data with known mean response (thick lines) and individual responses (points) showing the dispersion of the data. B,E: Estimates from the rm-ANOVA model for the mean group response (linear of quadratic). Points represent the original raw data. The rm-ANOVA model not only fails to pick the trend of the quadratic data (E) but also assigns a global estimate that does not take into account the between-subject variation. C, F: Estimates from the LMEM in the linear and quadratic case (subject: thin lines, population: thick lines) . The LMEM incorporates a random effect for each subject, but this model and the rm-ANOVA model are unable to follow the trend of the data and grossly bias the initial estimates for each group in the quadratic case (bottom row). This data was easy to run the addition of the used packages made running this code very easy. therefor I give it a 4.8 out of 5. "],["looking-ahead.html", "Chapter 5 Looking ahead 5.1 step 1 outlining data 5.2 Step 2 getting the pixel area", " Chapter 5 Looking ahead for this chapter we will be looking two years into the future and where I expect to be at that time. firstly I am not jet sure if I want to work or do a master after I graduate from the HU. So I could be either working had at a laboratory somewhere, it is important for me however that my work has a certain use for people. or I could be studding on my masters. as of right now I’m in my third year and still need to do an one year internship in Norway. for this Internship I will be working with spheroid assays. To familiarize myself with this assay i am going to design a process with which i can analyse these assays. for this project i am going to analyse some images from a spheroid assay and see if i can use R to contour the spheroid and give me the area it has grown over time. step 1 is for R studio to be able to draw the outline of the cells in an image. I plan to use the imager package for this and if necessary the magick package. I would also like to automate this but this can wait till after step 2. step 2 is for R studio to calculate the area of the of the cell (based on the given metadata aka: the zoom and starting size of the cells.) step 3 and last step is to R to able to this over every frame of a video/ be given a time to take an image. (if I say every 10 seconds or hour take the images) 5.1 step 1 outlining data first we load the images into our R environment # we downloaded the pictures from the #install.packages(&quot;imager&quot;) #install.packages(&quot;sf&quot;) library(sf) ## Linking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE library(imager) ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## ## Attaching package: &#39;imager&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## add ## The following object is masked from &#39;package:stringr&#39;: ## ## boundary ## The following object is masked from &#39;package:dplyr&#39;: ## ## where ## The following object is masked from &#39;package:tidyr&#39;: ## ## fill ## The following objects are masked from &#39;package:stats&#39;: ## ## convolve, spectrum ## The following object is masked from &#39;package:graphics&#39;: ## ## frame ## The following object is masked from &#39;package:base&#39;: ## ## save.image BLM_Day_1 &lt;- load.image(&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_1.png&#39;) BLM_Day_2 &lt;- load.image(&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_2.png&#39;) BLM_Day_3 &lt;- load.image(&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_3.png&#39;) BLM_Day_4 &lt;- load.image(&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_4.png&#39;) after loading the pictures we convert them into greyscale. Most of the used images should already be greyscale but just in case we can cover for it. BLM_Day_1_gray &lt;- grayscale(BLM_Day_1) BLM_Day_2_gray &lt;- grayscale(BLM_Day_2) BLM_Day_3_gray &lt;- grayscale(BLM_Day_3) BLM_Day_4_gray &lt;- grayscale(BLM_Day_4) Next we need to remove the background shading. In most pictures there will be lighter and darker areas in the background to remove these we use lm to find the linear trend. df_1 &lt;- as.data.frame(BLM_Day_1_gray) m_1 &lt;- lm(value ~ x + y,data=df_1) #linear trend im_1.g &lt;- BLM_Day_1_gray df_2 &lt;- as.data.frame(BLM_Day_2_gray) m_2 &lt;- lm(value ~ x + y,data=df_2) #linear trend im_2.g &lt;- BLM_Day_2_gray df_3 &lt;- as.data.frame(BLM_Day_3_gray) m_3 &lt;- lm(value ~ x + y,data=df_3) #linear trend im_3.g &lt;- BLM_Day_3_gray df_4 &lt;- as.data.frame(BLM_Day_4_gray) m_4 &lt;- lm(value ~ x + y,data=df_4) #linear trend im_4.g &lt;- BLM_Day_4_gray after finding the linear trend we fit this over the original image. I used image 4 as an example with the before and after picture. im_1.f &lt;- im_1.g-fitted(m_1) im_2.f &lt;- im_2.g-fitted(m_2) im_3.f &lt;- im_3.g-fitted(m_3) im_4.f &lt;- im_4.g-fitted(m_4) layout(t(1:2)) plot(BLM_Day_4_gray , main=&quot;Before&quot;) plot(im_4.f , main=&quot;After trend removal&quot;) so now that we have a nice even picture we are going to determine the threshold. the threshol shows us what part of the picture stands out the most hard lines and blotches of darker colour stand out. how much much the threshold will depend on how high or low you set it. in the example we have set the threshold to 25 , 15 and 5 % and we can examine it with the original picture. layout(t(1:2)) plot(im_4.f) paste0(c(25,15,5),&quot;%&quot;) %&gt;% map_il(~ threshold(im_4.f,.)) %&gt;% plot(layout=&quot;row&quot;) 5% is very low and you almost see noting of the lines, 25% gives a bit more detail than there is truly in the picture. 15% seems to be on the right track we are going to continue with that one and see if we can get a more detaild picture layout(t(1:2)) plot(im_4.f) paste0(c(17,15,13),&quot;%&quot;) %&gt;% map_il(~ threshold(im_4.f,.)) %&gt;% plot(layout=&quot;row&quot;) there are small differences in these pictures which concerns the small spots around the cell the closest we are going to get is probably the 17% one so we continue our analysis with that. We are going to save the 17% threshold into an new vector. Next we will convert this to a pixel set. we repeat the process of checking the threshold for every cell. it does not have to be perfect yet we can clean up in the next step im_1.t &lt;- threshold(im_1.f,&quot;3%&quot;) px_1 &lt;- as.pixset(1-im_1.t) #Convert to pixset im_2.t &lt;- threshold(im_3.f,&quot;4%&quot;) px_2 &lt;- as.pixset(1-im_2.t) #Convert to pixset im_3.t &lt;- threshold(im_3.f,&quot;7%&quot;) px_3 &lt;- as.pixset(1-im_3.t) #Convert to pixset im_4.t &lt;- threshold(im_4.f,&quot;18%&quot;) px_4 &lt;- as.pixset(1-im_4.t) #Convert to pixset plot(px_4) this is the result of the pixelset, the pixel set ecentially marks every pixel that was black in the threshold. now we can use the fill and clean command on this to clear of some of the pixels we don’t want and highlight the left over pixels. # png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_1_circle.png&#39;) # plot(BLM_Day_1) # fill(px_1,6) %&gt;% clean(3) # # png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_2_circle.png&#39;) # plot(BLM_Day_2) # fill(px_2,6) %&gt;% clean(2) # # png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_3_circle.png&#39;) # plot(BLM_Day_3) # fill(px_3,5) %&gt;% clean(0.5) # # png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_4_circle.png&#39;) # plot(BLM_Day_4) # fill(px_4,6) %&gt;% clean(1.5) plot(im_4.f) fill(px_4,6) %&gt;% clean(1.5) %&gt;% highlight here we can see the result of our plotting. its important that the complete cell is seround by the red border and that as many of the left over cells are included in that border. it is also important that it is disconnected from the discoloration in the bottom right. next we want to select the main body of our analysis we do this locating the middle of our image with the summarise function next we will put the outcome of this into the px.flood command this will select the whole white body that is part of the middle px_1 &lt;- fill(px_1,6) %&gt;% clean(3) px_2 &lt;- fill(px_2,6) %&gt;% clean(2) px_3 &lt;- fill(px_3,5) %&gt;% clean(0.5) px_4 &lt;- fill(px_4,6) %&gt;% clean(1.5) px_1_middle &lt;- imager::where(px_1) %&gt;% dplyr::summarise(mx=mean(x),my=mean(y)) px_2_middle &lt;- imager::where(px_2) %&gt;% dplyr::summarise(mx=mean(x),my=mean(y)) px_3_middle &lt;- imager::where(px_3) %&gt;% dplyr::summarise(mx=mean(x),my=mean(y)) px_4_middle &lt;- imager::where(px_4) %&gt;% dplyr::summarise(mx=mean(x),my=mean(y)) px_1 &lt;- px.flood(px_1, round(px_1_middle$mx),round(px_4_middle$my),sigma=.0001) px_2 &lt;- px.flood(px_2, round(px_2_middle$mx),round(px_4_middle$my),sigma=.0001) px_3 &lt;- px.flood(px_3, round(px_3_middle$mx),round(px_4_middle$my),sigma=.0001) px_4 &lt;- px.flood(px_4, round(px_4_middle$mx),round(px_4_middle$my),sigma=.0001) array(t(1:4)) ## [1] 1 2 3 4 plot(px_1) plot(px_2) plot(px_3) plot(px_4) now we have a pixel set with the outline of our cells. 5.2 Step 2 getting the pixel area with this pixelset we can calculate the pixels of the cells in this image px_1 ## Pixel set of size 703. Width: 125 pix Height: 106 pix Depth: 1 Colour channels: 1 px_2 ## Pixel set of size 1175. Width: 125 pix Height: 106 pix Depth: 1 Colour channels: 1 px_3 ## Pixel set of size 1634. Width: 125 pix Height: 106 pix Depth: 1 Colour channels: 1 px_4 ## Pixel set of size 5373. Width: 125 pix Height: 106 pix Depth: 1 Colour channels: 1 # percentage pixels of the total picture paste(round(sum(px_1) / (height(px_1) * width(px_1)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;5.31 %&quot; paste(round(sum(px_2) / (height(px_2) * width(px_2)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;8.87 %&quot; paste(round(sum(px_3) / (height(px_3) * width(px_3)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;12.33 %&quot; paste(round(sum(px_4) / (height(px_4) * width(px_4)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;40.55 %&quot; #percentage growth after day 1 paste(round((sum(px_1)-sum(px_1)) / (height(px_1) * width(px_1)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;0 %&quot; paste(round((sum(px_2)-sum(px_1)) / (height(px_2) * width(px_2)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;3.56 %&quot; paste(round((sum(px_3)-sum(px_1)) / (height(px_3) * width(px_3)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;7.03 %&quot; paste(round((sum(px_4)-sum(px_1)) / (height(px_4) * width(px_4)) *100, digits = 2) , &quot;%&quot; ) ## [1] &quot;35.25 %&quot; png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_1_circle.png&#39;) plot(BLM_Day_1) fill(px_1,6) %&gt;% clean(3) %&gt;% highlight png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_2_circle.png&#39;) plot(BLM_Day_2) fill(px_2,6) %&gt;% clean(2) %&gt;% highlight png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_3_circle.png&#39;) plot(BLM_Day_3) fill(px_3,5) %&gt;% clean(0.5) %&gt;% highlight png(file=&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_4_circle.png&#39;) plot(BLM_Day_4) fill(px_4,6) %&gt;% clean(1.5) %&gt;% highlight # colorise(BLM_Day_1,px,&quot;red&quot;,alpha=.5) %&gt;% plot # plot(px) # view(px) # imager::where(px) %&gt;% dplyr::summarise(mx=mean(x),my=mean(y)) # #BLM_Day_1_circle &lt;- load.image(&#39;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/vrij_opdracht/data/BLM_Day_1_circle.png&#39;) # #unique() # sum(px) # px # BLM_Day_1 "],["cv.html", "Chapter 6 CV", " Chapter 6 CV "],["connecting-and-using-an-external-database.html", "Chapter 7 connecting and using an external database", " Chapter 7 connecting and using an external database In this chapter I am going to show I used a external database (DBeaver) to store and process files. For this chapter we use the gapminder data of the dslabs package and the data from google about flu and dengue searches per country. firstly we load the flu dengue data from the repo and the gapminder data into data frames. the searches data contains a header which is needed to be skip. library(dslabs) Flu_data &lt;- read.csv(&quot;~/Rschool/Berrit-Github.github.io/data/flu_data.csv&quot;, skip = 11) Flu_data &lt;- as.data.frame(Flu_data) dengue_data &lt;- read.csv(&quot;~/Rschool/Berrit-Github.github.io/data/dengue_data.csv&quot;, skip = 11) dengue_data &lt;- as.data.frame(dengue_data) gapminder &lt;- as.data.frame(gapminder) Next we use R to tidy up the data a bit and make it compatible with each other. We do not yet make it tidy data this will come later. library(tidyverse) Flu_data_tidy &lt;- Flu_data %&gt;% pivot_longer(cols = -c(Date) , names_to = &quot;country&quot; , values_to = &quot;Weekly_searches&quot;) dengue_data_tidy &lt;- dengue_data %&gt;% pivot_longer(cols = -c(Date) , names_to = &quot;country&quot; , values_to = &quot;Weekly_searches&quot;) gapminder &lt;- gapminder %&gt;% pivot_longer(cols = -c(country , year, continent , region) , names_to = &quot;variable&quot; , values_to = &quot;values&quot;) we change the data from the flu and dengue data so it works with the gapminder data we do this by combining the data of the “Date” into “years” library(dplyr) Flu_data_tidy$Date &lt;- Flu_data_tidy$Date %&gt;% str_sub(1 ,4) dengue_data_tidy$Date &lt;- dengue_data_tidy$Date %&gt;% str_sub(1 ,4) Flu_data_tidy &lt;- rename(Flu_data_tidy , &quot;year&quot; = &quot;Date&quot;) dengue_data_tidy &lt;- rename(dengue_data_tidy , &quot;year&quot; = &quot;Date&quot;) Flu_data_tidy &lt;- Flu_data_tidy %&gt;% group_by(country, year) %&gt;% summarise(flu_searches = sum(Weekly_searches)) ## `summarise()` has grouped output by ## &#39;country&#39;. You can override using the ## `.groups` argument. dengue_data_tidy &lt;- dengue_data_tidy %&gt;% group_by(country, year) %&gt;% summarise(dengue_searches = sum(Weekly_searches)) ## `summarise()` has grouped output by ## &#39;country&#39;. You can override using the ## `.groups` argument. dengue_data_tidy$year &lt;- dengue_data_tidy$year %&gt;% as.numeric() dengue_data_tidy[is.na(dengue_data_tidy)] &lt;- 0 Flu_data_tidy$year &lt;- Flu_data_tidy$year %&gt;% as.numeric() Flu_data_tidy[is.na(Flu_data_tidy)] &lt;- 0 gapminder$country &lt;- as.character(gapminder$country) gapminder$year &lt;- as.numeric(gapminder$year) gapminder[is.na(gapminder)] &lt;- 0 now that the data is tidy and works with each other we write them back to a new map. write.csv(Flu_data_tidy , &quot;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht7/Flu_data_tidy.csv&quot; , row.names = FALSE) write.csv(dengue_data_tidy ,&quot;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht7/dengue_data_tidy.csv&quot; ,row.names = FALSE) write.csv(gapminder ,&quot;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht7/gapminder_data_tidy.csv&quot; , row.names = FALSE) write_rds(Flu_data_tidy , &quot;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht7/Flu_data_tidy.rds&quot;) write_rds(dengue_data_tidy ,&quot;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht7/dengue_data_tidy.rds&quot;) write_rds(gapminder ,&quot;C:/Users/Dell/Documents/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht7/gapminder_data_tidy.rds&quot;) Next we are going to connect to the server and save the data there we use DBeaver for this. library(RPostgreSQL) library(devtools) library(remotes) library(RPostgres) library(DBI) dbWriteTable(con, &quot;flu_data&quot;, Flu_data_tidy) dbWriteTable(con, &quot;dengue_data&quot;, dengue_data_tidy) dbWriteTable(con, &quot;gap_data&quot;, gapminder , overwrite = TRUE) After we have saved the data into the data base we can see out newly made databases in DBeaver we can check t contents of these dataframes with SQL code in dBeaver next we are going to combine all three sets in one big table by using the connection command in the header we are connected to our data base in DBeaver. we use SQL language in rstudio to make the changes. In this figure we can see all the colums that are in the gap_data table Now that we combined the data into a new table we call this back to R using the con vector to connect to the database while coding in R language. after pulling back the data we can also pivot the data to make it tidy this will make it easier to work with while plotting. to get an idea of the data we can use summary to tell us statistics such as mean or sd. for example the average amount of flu searches in 2013 was 1.9229923^{4} (r summary(filter(all_data, year == 2013 )flu_searches)[4] ) and the amount of dengue searches was 8.8201(summary(filter(all_data, year == 2013 )dengue_searches)[4]) first it would be interesting to see which country searches flu and dengue the most to make this more even we will be looking at the amount of searches per 100 citizens the dengue searches were lower and we need look for searches per 1.000.000 citizens. all_data &lt;- dbReadTable(con, &quot;all_data&quot;) all_data_2 &lt;- all_data[, c(1:7, 10 , 11)] all_data_2 &lt;- pivot_longer(all_data_2, cols = -c(country , year), values_to = &quot;values&quot; , names_to = &quot;variable&quot;) filter(all_data , flu_searches &gt; 0) %&gt;% filter(country == &quot;Austria&quot; | country == &quot;Bulgaria&quot; | country == &quot;Canada&quot;) %&gt;% ggplot( aes(x = year , y = (flu_searches/population)*1000 , group = country , colour = country, label = country)) +geom_point() + geom_line() + labs(y = &quot;amount of flu searches per 1000&quot;) filter(all_data , dengue_searches &gt; 0) %&gt;% ggplot( aes(x = year , y = (dengue_searches/population)*10000000, group = country , colour = country)) +geom_point() + geom_line() + labs(y = &quot;amount of dengue searches per 10.000.000&quot;) this .. and this country searched flu the most lastly we wanted to see which regions googles flu the most the most the most. to see if there is any connection between place and the searches. filter(all_data , flu_searches &gt; 0 &amp; gdp &gt; 0) %&gt;% ggplot( aes(x = region , y = (flu_searches/population)*100, group = country , fill = region)) + geom_col() + labs(y = &quot;amount of flu_serches&quot;) + theme(axis.text.x=element_text(angle = 45, hjust = 0.75)) "],["easy-ic50-plotting-package.html", "Chapter 8 easy IC50 plotting package", " Chapter 8 easy IC50 plotting package I have made a package with wich IC50 data can easily be plotted it can be downloaded here devtools::install_github(&quot;Berrit-Github/easyIC50plotting&quot;) ## Skipping install of &#39;easyIC50plotting&#39; from a github remote, the SHA1 (18dd00d2) has not changed since last install. ## Use `force = TRUE` to force installation "],["params.html", "Chapter 9 Params", " Chapter 9 Params In this chapter we are going to make a rmarkdown with some parameters after adding our params we have the next html file in this we will show the covid cases and deaths of croatia of august 2021 #make screenshot of parameterized Rmarkdown webshot::webshot(&quot;data/portfolio_opdracht9.html&quot;, &quot;data/portfolio_opdracht9.png&quot;) next we can render this rmarkdown in two ways with the first way we can render the new file by setting the params like so #rmarkdown::render(&quot;~/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht9.Rmd&quot;, params = list(country = &quot;Denmark&quot;, year = 2021, month = 8), output_file = &quot;denmark_8_2021&quot;) the second way is to replace the params with ask #rmarkdown::render(&quot;~/Rschool/dsfb2_workflows_portfolio/portfolio_opdracht9.Rmd&quot;, params = &quot;ask&quot;, output_file = &quot;denmark_8_2021_params_ask&quot;) this will open a webpage with sliders and a list of country’s example of the slider webshot::webshot(&quot;data/denmark_8_2021_params_ask.html&quot;, &quot;data/denmark_8_2021_params_ask.png&quot;) after looking at the graphs in the new markdown you can see the data has changed arcorindgly to the given params "],["introduction-projecticum.html", "Chapter 10 introduction projecticum", " Chapter 10 introduction projecticum when an organism ages multiple bodily factors will decrease in function. eyesight , hearing and muscle strength are all examples of this. In case of muscle strength in which muscles mass and function decreases is called sacropenia. Normal and mostly young muscle react on the environmental stimuli to grow such as insulin and leucine. when stimulated they make muscle protein this processes is called muscle protein synthesis (MPS). This process needs to be up kept to counter natural muscle decay. In aged individuals it is shown that due a resistance to stimuli such as insulin and leucine and elderly individuals perform little muscle training, that the balance between MPS and natural muscle decay is in favor the decay of muscles and so causes sacropenia (Miriam van Dijk et al. (2016)). Further research (F. J. Dijk et al. (2018)) shows that leucine is a important factor in the pathway of MPS. The first evidence of this was by feeding young rats a single meal with a high leucine concentration after inspecting the muscles of these rats they showed a higher amount of MPS. additional it also proved that rats who were fed a higher dose of leucine showed more MPS than those with a low dose of leucine revealing a connection between the concentration and the reaction. therefore it showed promise to possibly combat sacropenia in older rats. furthermore luecine controls MPS by being able to stimulate the Mthor pathway the mthor pathway can later stimulate the other other factors such as p70s6k and 4E-BP1 these factors influence the mRNA translation which results in more MPS in muscle cells. These factors were the start of many experiments looking at the different effects of leucine, such as what other factors are involved, different administration methods, and the effects of one or more doses spread over a longer period of time. In the following research it turned out it was not as easy as firstly thought. It shows that older individuals have become more resistent to leucine with age (Miriam van Dijk et al. (2017)) &amp; (Boirie and Guillet (2018)) In order for leucine to work the fed the older rats a higher dose than one would a normal rat and it was seen after eating this meal the rats did have a higher MPS then before eating the meal (Rieu et al. (2003)). One of the most used proteins in this type of research is the whey protein. this protein is quick to digest and is rich with the leucine amino-acid. this protein has a very positive effect on the MPS. Currently a research team by nutricia is looking into how leucine contributes to MPS and if there are plant based alternatives which can have similar effects as the whey protein. after giving the mice one of the experimental plant based protein their aminoacid values are measured. All of these value are compared. The results of this experiment gives a mountain of data to be decoded. to visualize all of this data so it would be easier to compare nutricia called in the help of the HU (hogeschool Utrecht) and their data science students. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
